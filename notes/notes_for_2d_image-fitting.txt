NOTES ON 2D IMAGE FITTING:

[x]Use Craig M.'s mpfit C code for testing/experimentation?
	-- could later add use of Diff.Evoln. solver
	Limits: lower, upper; presence or absence of both; hold fixed


[x]"Model" (Function+Data) Object:

	Holds:
		Input image (perhaps as 1D vector)
		Error/weight image (")
		
		Pointer/slots for functions
		
		Function for computing & returning 1D vector of deviances (what mpfit
		wants: dy[i] = (y[i] - f)/ey[i] )
	[mpfit: could be passed as the "private" data structure]


	Array to hold function objects?
		-- number of functions
		-- knowledge of how many parameters/function, and in which order
		(i.e., will receive one long vector of trial parameters; need to know
		how to chop it up and send off individual parameter sets to appropriate
		individual functions
			-- copy parameters into a C++ vector of parameters (i.e., pointer to
			arrays of double, each array corresponding to the appropriate-sized
			parameter array for the given function)
		-- pointers to actual function objects
		-- * Use C++ vector

[x]Function objects:
	-- parameters we will almost always have:
		x0, y0  [not necessary for e.g. sky background]
		PA [not for circular things]
		ellipticity [not for circular things, edge-on analytic disks, etc.]
	-- Program should handle multiple instances with different parameters 
	(e.g., Sersic + Sersic), as GALFIT does
	-- for now, handle x_im,y_im --> r, PA calculations within the function
	object (not needed for circular components); later, we can think about
	maybe handling some of it outside (e.g., function object has flag that
	lets model object know to do those calculations...?)


Alternate approaches to adding function contributions:
	For each pixel, add all function contributions
	OR: For each function, allocate an image, compute function values, then add
	all images together
		-- disadvantage: slightly more overhead?
		-- disadvantage: uses more memory
		-- advantage: less overhead in switching back & forth between functions?


Image as 1D vector:
	Craig M.'s MPFIT2DFUN IDL code works with 2D arrays (1 each for x, y, and actual
	values z), but turns the 2D array of deviances into a 1D array (the latter is what
	gets returned).  (In more detail: MPFIT2DFUN implements a wrapper function as the
	"user function"; this wrapper function takes the real user function, applies it
	and calculates the deviances, then passes the 1D array of deviances back as its
	return value.)
	
	A simple approach:
		1. Convert input image (and input error/weight image?) into 1D vector
		2. Store row and column sizes, so we can reconstruct x,y pixel coordinates
		when we step through the 1D vector
		3. Calculate deviances via something like this: [NOTE: this needs to be
		coordinated with whatever scheme we use to convert the original image, so
		that we step smoothly through the 1D vector...
		
			for (i = 0; i < modelObject->nRows; i++) {
				y = i + 1;   // so x,y are 1-based, not 0-based
				for (j = 0; j < modelObject->nColumns; j++) {
					x = j + 1;
					modelVal = modelObject->modelFunc(x, y, paramsVector);
					ii = i*nRows + j;   // index into 1D vector
					deviances[ii] = (modelObject->DataVect[ii] - modelVal) / modelObject->ErrVect[ii];
				}
			}

	According to http://en.wikipedia.org/wiki/Row-major_order, this format is
	row-major, which is the C default.  The nice thing is that such an array can,
	in princple, be fed to FFTW *as is* -- or at least, we can create a 1-D
	pixel array of fftw_complex values with exactly the same structure as the
	input image array.
		-- in FFTW terms, n_0 = nColumns, n_1 = nRows [[I think...]]
		-- on the latter point: if we do the faster, real-based FFT, then things
		are a little more complicated; we can postpone figuring this out till
		later

From http://www.covig.imi.aau.dk/publications/MDobroczynski2DFFT2006.pdf:
"Let us assume to have N*M matrix. Row-major format simply puts all rows
(N) of the array one after another (M times), so that as a result we
have a N*M vector. Similar situation is for column-major format â€“ but
this time columns are put one after another (column-major is more
popular among Fortran programmers)."



Coordinates:
	Standard IRAF/DS9 approach is: *center* of first pixel is 1.0,1.0;
	sub-pixel coordinates run from (x - 1).5 on L edge to x.0 in center to x.5 on R edge,
	(y - 1).5 on bottom edge to y.0 in center to y.5 on top edge.
	
	We're using this approach. (Verified 8 March 2010 by inspecting makeimage output
	image in iraf.)  Here's the code used in model_object.cpp to step through image
	pixels -- note that the very first pixel has coordinates (x,y) = (1.0,1.0):
	
  for (int i = 0; i < nRows; i++) {   // step by row number = y
    y = (double)(i + 1);              // Iraf counting: first row = 1
    for (int j = 0; j < nColumns; j++) {   // step by column number = x
      x = (double)(j + 1);                 // Iraf counting: first column = 1
      newVal = 0.0;
      for (int n = 0; n < nFunctions; n++)
        newVal += functionObjects[n]->GetValue(x, y);
      modelVector[i*nColumns + j] = newVal;



Possible elaborations:
	Input text file which hold model specifications, initial guesses, limits
	Python script to convert command-line arguments into text file?  Or Python script
	to read text file and convert them into command-line arguments?

Experimenting with possible options for specifying limits:

FUNCTION   Sersic-1D   # here is a comment
n       2.0   fixed
mu_e   24.0   20.0/25.0   NO
r_e     5.0   1.0:10.0

n       2.0   0.0--10.0
mu_e   24.0   (20.0,25.0)
r_e     5.0   1.0,10.0      CURRENTLY USING THIS ONE



Version of function objects with and without generalized ellipses (see
Eqn.3 of Peng+2002, or Eqn. 21 of the Galfit-3.0 README for a nice
example of how to get r given x',y',q,c, assuming that x' and y' are in
the principle-axes-of-the-ellipse coordinate system).
	--- From Galfit-3.0 README: "C0 should not be used until a best fitting 
	ellipsoid model has been found." [i.e., it's best to fit with normal
	ellipses, and then re-fit from that fit using generalized ellipse]

Sky background: Chien Peng notes (in the readme file accompanying galfit
2.0) that it's best to keep the sky "component" fixed to the best
measured value: " as a rule of thumb, always determine the sky value
independently and hold it fixed in the fit. Allow the sky parameter to
vary only when desperate."
	-- So it's plausible to have "sky component" to allow users to work with
	non-sky-subtracted images, with the proviso that it's best to measure
	a value, and then keep that value fixed.


QUESTIONS AND ISSUES:

	[] Convolution vs non-convolution:
		-- in principle, some "functions" (components) might not need convolution;
			-- sky background
				-- probably makes no difference if we include this; OR we should
				include it pre-convolution, since that's partly what happens in
				reality (upper-atmosphere scattered light or glow gets convolved
				with astronomical source light in lower atmosphere?)
			
			-- pure point source
				-- but this requires that user supply a point source object (or that
				we allow specification of a PSF function)
				-- note that GALFIT 2.0 avoids this by recommending that user specify
				a very narrow Gaussian as point-source
				-- [Apr 2010] Experiments suggest that we can get a rather good
				approximation of a scalable point source by convolving a Gaussian of
				sigma = 0.1 or 0.2 [e.g., with limits = "fixed"] with the PSF
			
		Conclusion (so far): not necessary to work with separate convolved and
		non-convolved components [but keep this "issue" here in case we think of
		more possible exceptions]


	[ ] Generation of PSFs
		-- mode where user specifies an extra config file which describes a PSF
		image (e.g., Gaussian, or Moffat fn., or anything that makeimage can handle)
		-- create a temporary ModelObject instance to generate the PSF image,
		but then keep it in memory (assign to psfPixels array) and delete ModelObject
		instance
		-- config file should ideally specify image size
		-- sanity/input check: requested PSF image should be square, odd-sized,
		with X0,Y0 in image center


	[ ] Option to do convolution on oversampled grid around object center?
		-- e.g., in 10x10 grid around object center, oversample object *and* PSF,
		do convolution using oversampled image, then resample to input scaling
		and past into main image.
		-- does GALFIT do something like this? [No, at least according to the
		documentation when it uses an oversampled PSF, it does this for the entire
		image [or the entire PSF convolution box?]]
		
		Issues/things to think about:
			-- Secondary stage to apply CTE or similar pixel diffusion kernel,
			after resampling to original pixel scale?
			-- Need to translate pixel coordinates to oversampled grid -- look at
			current pixel-subsampling code in function objects...
			-- Do we need to rescale things like pixel sizes (prob. not -- we just
			work with same pixel dimensions on a smaller subgrid, as we do for
			pixel subsampling)
			-- Do we need to rescale intensities (counts/pixel, etc.)?
				-- current pixel subsampling just divides by number of subpixels
				to normalize; we can probably do the same (i.e., sum up the subpixels
				to form a final original pixel, then divide by number of subpixels)
			
			[x]-- Need separate subimage model vector (but not error, weight, mask, etc.),
			appropriately padded
			[x]-- Need separate oversampled PSF image vector
			[x]-- Need separate Convolver component
			
			-- Insert code into ModelImage::CreateModelImage *just* before end of
			current code (so we do all the normal image creation & convolution first,
			and then replace the central NxM pixels with the rebinned subset)
		
			[x]-- command-line option to specify region to oversample and oversampled PSF
			to use, along with oversampling factor
				1. bounding box of oversample region (coordinates + width?)
				2. oversampled PSF file (FITS file)
				3. oversampling factor (single integer)
			-- optional CTE diffusion kernel
			-- possible extreme scenarios:
				1. Do oversampled PSF on central region + *no* PSF convolution for rest of image
				2. Do entire image with oversampled PSF
					-- This requires *skipping* the normal image creation!
						
			-- Possible later option: allow user to specify multiple oversampling regions,
			*as long as they are all the same size & shape* (e.g., 10x10 or 25x25)
				-- this allows us to re-use the same Convolver component (oversampled PSF +
				specific size for oversampled sub-image) with different oversampled
				subimage model vectors
			
		
		-- Possible future architecture: plug-ins for post-convolution processing
			e.g., after convolution, an optional stage where one (or more?) functions
			is called (or objects which get passed the image as input) which can perform user-specified
			post-processing (such as applying a CTE kernel)
				-- need to allow user to specify which plug-in to use
					-- command-line option? line in main config file?
				-- need to figure out how to allow user to specify input information
				for their plug-in: special configuration file? (this would require
				config-file option-parsing code in the plug-in, which is probably OK)
				-- if more than one plug-in: main program builds up list of miscellaneous 
				configuration files, and then queries each of the user-specified plug-ins whether
			-- Could also replace our current convolution architecture with a more plug-in-style
			approach, so that the user to could specify standard single-PSF convolution *or*
			some kind of spatially-varying PSF approach
		
		-- Post-convolution processing: CTE, etc.
			One major form of post-convolution processing would be incorporating the effects
			of CTE (charge-transfer efficiency) for CCDs or IPC (interpixel capacitance) for
			IR arrays
			
			Really complicated examples would account for geometric distortion correctsions, etc.
			E.g.: convolution (with spatially varying PSF?) at image plane, then CTE/IPC
			effects, then geometric-distortion correction (e.g., HST's ACS and WFC3)
		
		[] Test spline-subsampling of PSF by comparing result of subsampling
		TinyTim standard-sampled PSF to TinyTim-generated subampled PSF



void ModelObject::AddPSFVector(int nPixels_psf, int nColumns_psf, int nRows_psf,
                         double *psfPixels)
{
  assert( (nPixels_psf >= 1) && (nColumns_psf >= 1) && (nRows_psf >= 1) );
  
  nPSFColumns = nColumns_psf;
  nPSFRows = nRows_psf;
  psfConvolver = new Convolver();
  psfConvolver->SetupPSF(psfPixels, nColumns_psf, nRows_psf);
  psfConvolver->SetMaxThreads(maxRequestedThreads);
  doConvolution = true;
}

in SetupModelImage:
  if (doConvolution) {
    nModelColumns = nDataColumns + 2*nPSFColumns;
    nModelRows = nDataRows + 2*nPSFRows;
    psfConvolver->SetupImage(nModelColumns, nModelRows);
    psfConvolver->DoFullSetup(debugLevel);
    nModelVals = nModelColumns*nModelRows;
  }


	[] Spatially varying PSFs and other exotica
		Hartung+2012, "GPU Acceleration of Image Convolution using Spatially-varying Kernel"
		http://arxiv.org/abs/1209.5823
		-- Notes that convolution via FFTs *cannot* be done when PSF is spatially varying
		
		C. Alard, "Image subtraction using a space-varying kernel," Astron. Astrophys. Suppl. Ser., vol. 144, pp. 363-370, 2000.
		C. Alard and R. H. Lupton, "A Method for Optimal Image Subtraction," The Astrophysical Journal, p. 325, 1998.
		J. P. Miller, et al., "Optimal Image Subtraction Method: Summary Derivations, Applications, and Publicly Shared Application Using IDL," Publications of the Astronomical Society of the Pacific, vol. 120, pp. 449-464, 2008.
		


	[] Create a subclass of FunctionObject which includes pixel-subsampling
	code for elliptical objects?
    	-- i.e., the current [26 Mar 2010] code in Sersic::GetValue and Exponential::GetValue
	    is identical, and would be the same for any other elliptical object
	    (a generalized-ellipse object would have slightly different code, but all
	    such objects would presumably share it)
	    -- note that we would need a different function for e.g. edge-on disks
	    (which are not elliptical, and which include distance from mid-plane as
	    a subsampling criterion)
	    
	    
	    
	[] Possible option for 1-D fitting: use 2d function objects, with: x and x0
	specified so that x - x0 = r; y = y0 = 0.0; q = 1.0, PA = 0
		-- advantage is that we don't need to write separate function-object
		modules for 1-d case
		-- subsampling:
			-- use as in 2D case if profile is equivalent to, e.g., 1-pixel-wide
			cut or something similar
			-- turn subsampling off if fitting something that isn't equivalent
			to a 1-pixel-wide cut
		-- We will still need a separate ModelObject class, to handle things
		like separate x-coordinate and y-value vectors, possible missing data;
		convolution (e.g., we need to handle special cases like excluded data,
		data profile that starts at r=0, etc.)


	[] Gain maps?
	-- How would we handle the case of a mosaic image, where some pixels have
	more exposure depth than others? Usually a "weight map" is produced, though
	the units of this are obscure. (Also, weight map tells you nothing about
	flux-based errors: the center of a bright star in a low-weight region should
	still have larger sigma than a background-sky high-weight region.)
	For chi^2, one could conceivably make do with a sigma or variance map
	
	For Cash statistic, this doesn't work -- how can we recover proper Cash-statistic
	"weighting" in this case?
	
	Think of each pixel as a completely separate observation: effective gain from
	instrumental gain, cumulative exposure time, etc.
	Assume a mosaic image has been scaled to a common level. "Effective gain" then
	varies from pixel to pixel (or at least between regions of different
	exposure

	low-weight pixel (w=1):  10 (original counts = 10)
		multiply data and model values for this pixel by 1 for Cash-stat
	high-weight pixel (w=3): 10 (original counts = 30)
		multiply data and model values for this pixel by 3 for Cash-stat
	
	
	
	[x] How to handle convolution when object/component center is close to edge
	of image? (ideally, we would need to make the model image larger, do
	convolution, then crop back to data-image size)
	

	[x] How best to combine multiple functions into model object
	
	[x] How to map image x,y to function parameters (R, or R + theta, or ...)
		-- look at modelimage code
		-- handled by function objects?
	
	[x] How to allow for separate x0,y0 for each component vs same (adjustable)
	x0,y0 for all components
		-- Note that in principle we could have 2+ *sets* of linked components
		(e.g., simultaneously fitting bulge+disk+... for two overlapping galaxies...),
		where each set has a common x0,y0
		-- Let ModelObject keep parallel set of parameters
		-- Change FunctionObject::Setup() interface to *require* x0, y0 for all
		components as separate from main parameter vector (functions can ignore
		the x0, y0 values if they want to); then have a setting w/in ModelObject
		which tells it whether or not all components share same x0,y0
			e.g., x0,y0 = paramsVector[0],paramsVector[1] in the case of shared
			x0,y0 (pass paramsVector[2:] as input to function objects); in case
			of individual x0,y0, we either have all the x0,y0 pairs in the first
			part of paramsVector, or we keep them at the beginning of each
			individual-comonent parameter block (w/in paramsVector), adding 2 to
			the offset when we pass paramsVector to the function object...

		Remember: params has to contain all the unique x0,y0 pairs (but not more
		than that), since the minimization code needs to adjust them.
		Two options:
			==> store all x0,y0 at beginning of params?
				-- nSets [added to offset as index into params vector]
				-- something that 

			==> store x0,y0 in order of functions & set?
				-- better for simple printing of output
				-- need something to track whether we've reached a new set or not
				-- 
			
		A possible setup:
			Vector in ModelObject: delta-Nparam between different sets of components
			(simplest case: vector = [0]
			case of exp. disk + exp. disk:vector = [0,5]
			Vector of coordinates: one x,y pair per set of components
			
  for (int n = 0; n < nFunctions; n++) {
    if newXYVals[n] == true {
      x0 = params
    x0 = x0Vect[n];
    y0 = y0Vect[n];
    functionObjects[n]->Setup(x0, y0, params, offset);
    offset += paramSizes[n];
  }

			Two lines in config file per set:
			X-COORD   xxx.x   x1,x2 [or "fixed"]
			Y-COORD   yyy.y   y1,y2 [or "fixed"]
			(followed by 1+ FUNCTION <funcname> "declarations" + associated parameters)


	[] Possible complication/expansion to X0/Y0 function groups: can we create an
	interface and internal mechanisms to handle "shared" parameters
		-- function group is then defined as a set of functions *and* the shared
		parameters (X0, Y0, plus ...?)
		-- e.g., we might want to specify that all components in a group share the
		same PA as well as X0 and Y0
		-- note that, e.g., shared PA would reduce nFreeParams (this is correct & desired)
	
	
	[] Separate, small program which just generates appropriate template parameter
	lists for each (or all) function?  E.g., use can call it, then copy & paste the
	output into a config file.  Should accept as input the name of a function, or
	the generic "all" (or maybe "all" is default mode?).
		
	[] TO TEST: Does fixing nominally free parameters slow things down compared
	to doing a fit with the parameters excluded?
		-- i.e., say we decide to keep x0,y0 fixed: is there a speed advantage
		to having a separate function for which they are not free parameters?
		(e.g., L-M code doesn't waste time calculating derivates w.r.t. to those
		parameters)



[X]2014: Testing detection and masking of NaN values in images:


Images from Giulia S.:
[from my email to her:]
> Could you supply me with examples of an input image and a sigma image,
> with NaN values -- and a mask image that should mask out the NaN values but
> apparently doesn't -- that are giving you trouble (one each would be enough), so
> I can use them to test improvements?

Galaxy image: <ic1459_op_mos.fits>

Sigma image: <ic1459_op_unc.fits>

Mask: <ic1459_op_mos_edgestarmask_extra.fits>


relevant uses of isfinite():
VetDataVector
CheckWeightVector


AddErrorVector:
	calls CheckWeightVector; exits w/ error if "false" is returned

FinalSetup
	calls VetDataVector; exits w/ error if "false" is returned
		




OUTLINE OF PROGRAM:

1. Parse user input

2. Read in other input files: model spec; initial parameters; etc.

3. Create model object

4. Create function objects & assign to model object

5. Read input image & transform to 1D vector (store nRows, nCols)
	-- Later: read in error/weight map; mask
	[could be done after step 2]

6. Assign input-image vector [& other data to be used in fit] to model object

7. Do fitting

8. Report results of fit




** PLAN/OUTLINE:

[x] Work out initial test data: very small image with e.g. Gaussian or exp.disk

[x] Prototype code for reading in image (using cfitsio) and unrolling it into
1D vector
	[x] Convert this code to a function

[x] Prototype code for function object - something simple like exp. disk

[x] Prototype code for model object holding data, function objects

[x] Prototype chi-square/deviance code for use with mpfit

[x] Prototype code for having multiple function objects in ModelObject

[x] Prototype code for handling multiple parameter sets:
	-- parameter sets for each function object
	-- function/method which disassembles the mpfit-generated array of
	new parameter values into individual parameter sets, one for each function
	object, in the correct order.
	-- stored in ModelObject as C++ vector of double *
	-- for each individual parameter array:
		array of double
		nParams
	-- [Later?] function/method which bundles up initial parameter guesses, in the
	right order, into the combined 1-D initial-params array for mpfit
	-- END RESULT: much simpler to pass entire parameter vector to each function
	object, along with index offset...

[x] Prototype command-line processing
	-- Try using Kishan Thomas' anyoption.cpp class

[x] Prototype code for reading in an error (variance?) image & using it in
ModelObject
	[x] Start with image with all pixels = 1; result should be same as fitting
	without noise image

[x] Prototype a separate program (or command-line options?) to generate test images of functions
	[x] Prototype code for saving an image: use original input image
	[x] Prototype code for saving a model image stored in ModelObject

[x] Prototype code for selecting function objects & putting them into ModelObject
	-- e.g., given vector of strings with function names
	-- should do basic sanity check: total # parameters in user-supplied parameter
	vector = total # parameters for all functions (theModel->GetNParams())?

[x] Refine makeimage code:
	[x] Add code to read function specifications from config file
	[x] Add command-line options (and/or config file?) for image dimensions

[x] Prototype code for PA-aware function objects (e.g., exp. disk)
	-- angletest.c has working code for testing (x,y) --> (x',y') rotation transform.

[x] Prototype code for setting up model object based on user input?
	-- see www.cprogramming.com/tutorial/string.html

[x] Prototype code for read & using mask image
	-- note that our "standard", ellipse-compatible masks are "good = 0, bad > 0";
	the versions generated by sdssproc.py are integer images.
	-- start off with *integer* masks; later, add ability to read floating-point masks
	[x] Prototype checking and reading in mask image
		-- check to make sure it is same size as image!
		-- add command to print mask image to screen
	[x] Prototype code to convert mask values
		-- best for us to use: good = 1, bad = 0
	[x] Prototype use of mask image in ModelObject

[x] Add function-selection code (currently used by makeimage) to imfit

[x] Protoype code for reading and applying parameter limits
	[x] Prototype code to read & store parameter limits from config file
	[x] Prototype code for setting up mpfit parameter limit structure

[x] Prototype code for multiple component sets
	-- see test_config_sets.dat for example of format
	[x] Start with test_parser.cpp
	[x] Prototype modifications of makeimage to generate multiple sets
	[x] Generate & inspect test images with makeimage
	[x] Set up basic regressions tests for makeimage
	[x] Prototype code for multiple component sets with imfit

[x] Add checking of parameters for "nan" values

[x] Prototype code for saving output model image

[x] Prototype checking & reporting of mpfit error codes

[x] Prototype function-testing code and files
	[x] Prototype command-line template for 1-D image
	[x] Prototype config-file template
	[x] Prototype Python code: read in image via pyfits; plot vs actual function
	[x] Write short text file summarizing testing

[x] Prototype chisquare() member function for ModelObject()
	-- i.e., something for diff'l evoln. to call; computes non-overflowing sum
	of squares of individual deviances
	[x] Prototype basic chisquare function
	[x] Prototype internal allocation & de-allocation of deviances vector and
	simpler chisquare function

[x] Prototype code for FFT convolution in makeimage
	[x] Prototype PSF reading in makeimage
		[x] Code for getting PSF image name (and check for existence)
		[x] Code to read PSF image into a pixel vector
	[x] Prototype skeleton/stub-function use of convolver.cpp by ModelObject
	[x] Prototype use of convolver.cpp code for PSF storage, shifting by ModelObject
	[x] Prototype convolution in makeimage
	[x] Prototype incorporating convolution into ModelObject

[X] Prototype flat sky model function
	-- NOTE: for future Monte-Carlo simulations -- it might be easier to
	use a non-sky-sub. image and tweak the best-estimate sky value used
	in a fixed flat-sky model

[x] Prototype "psfconvolve" program
	[x] Prototype ShiftAndPadPSF() function: takes PSF image & push into corner
	w/ correct wrap-around
	[x] Test ShiftAndPadPSF() function with external PSF FITS images
	[x] Do further tests with convolving external FITS images
	[x] Prototype simple program which reads in image & PSF and saves convolution
	[x] Implement trimming of final output image
	[x] Prototype convolver.cpp module to contain PSF convolution code
	[x] Do initial tests of convolver-based psfconvolve






[.] Prototype use of differential evolution
	[x] Prototype bare-bones DE solver
	[x] Prototype use of DE by profilefit
	[x] Prototype use of DE by imfit
	[x] Investigate use of parameter bounds internally
		-- check to see how code in ~/coding/imfit/de_storn/de4_0_orig.cpp can or should
		be applied to DESolver.cpp
	[] Prototype user selection of DE parameters (F, CR, strategy, Npop multiplier)
		-- command-line switch? text-file?
	[x] Investigate possible convergence/stop criteria for DE
		[x] modify DE code to print out chi^2 to higher precision and/or delta-chi^2
		-- e.g., if best chi^2 value does not change by more than epsilon and/or
		after N generations, then declare a halt...
	[] Investigate possible ignoring of fixed parameters in DE
		-- i.e., restrict DE's population & mutation/crossovers to just the free
		parameters, re-inserting the fixed parameters only when calling the chi-square
		function

[] Prototype basic checking of initial parameter values (and limits?)
	-- Mainly/only for startup (sometimes it's useful to know that best-fit
	values are "physically impossible" ...)
	-- FunctionObject classes should have a method which checks a passed-in set
	of initial parameter values for sanity; reply with String error messages if
	there are problems
	-- Brainstorm how to handle parameter limits (two for each parameter, but
	some/many parameters may not have limits
	
[] Prototype error-printing functions
	-- e.g., something like what's in Kernighan & Pike's _The Practice of
	Programming_
	[] Prototype an error-printing function
	[] Prototype an error-printing-and-exit function
	[] Add error-printing function use to rest of code

[.] Prototype elliptical Moffat profile function
	[x] Prototype 1-D Python function for use in testing subsampling
	[x] Prototype C++ Moffat function
	[] Generate Moffat-function test images & compare 1-D cuts with 1-D Python function
	
[] Investigate fine-tuning of differential evolution
	-- e.g., "meta-optimization" using SwarmOps?

[] Improvements to noise-image generation
	[x] Check GALFIT README against our current code
		-- are we using the sky value incorrectly? [ANSWER: YES]
	[x] Prototype option to output weight map
	[] Generate trial sigma maps using GALFIT and compare them with our
	   version using the same input galaxy image
	<>[x] Prototype use of an optional "n_combined" input parameter
		-- number of combined images used to generate input image
		-- true flux (and pre-sub. sky background, if any) = n_combined*image_flux
		-- total readnoise^2 = N * rdnoise^2 (add individual rdnoise inputs in
		quadrature)
		-- preliminary calc. suggests that all we need to do is multiply
		chi^2 by n_combined
		[x] Prototype command-line setting of "n_combined"
		[x] Prototype internal storage of "n_combined"
		[x] Prototype passing "n_combined" to ModelObject
		[x] Prototype use of "n_combined" in ModelObject
	[-] Prototype use of an optional "t_exp" input parameter?
		-- GALFIT README indicates that GALFIT does *not* use EXPTIME for
		noise calculation (instead, user is advised to multiply the image
		by EXPTIME to convert counts/sec to counts). The only thing that
		GALFIT uses EXPTIME for is calculating magnitude values, since it
		assumes that an input zero point is for count/sec
		-- total exposure time *iff* input image has units of ADU/sec
		-- true flux (and pre-sub. sky background, if any) = n_combined*image_flux
		-- problem: doesn't affect total readnoise calculation (unlike n_combined),
		so effect on noise
		-- OK, we'll take the same approach as GALFIT and *not* use t_exp

[.] Test possible sub-pixel generation schemes (Python)
	[x] Write Python code to do sub-pixel sampling
	[x] Test effects of different subsampling for exponential (vs scale length)
	[] Test effects of different subsampling for Sersic (vs r_e, for different n)
	[] Make some notes on plausible subsampling schemes

[.] Implement pixel subsampling
	[x] Implement Chien Peng's Sersic subsampling algorithm in func_sersic.cpp
	[x] Implement Chien Peng's Sersic subsampling algorithm in func_exp.cpp
	[] Check func_sersic.cpp subsampling: generate circular Sersic image with
		center in center of pixel; extract cut through center; compare with
		Python-generated subsampled 1-D vector (100x100 subsampling for each
		1-D pixel)
	[] Implement Chien Peng's Sersic subsampling algorithm in func_gauss.cpp

[.] Prototype code for checking inputs:
	[x] input files should exist
	[x] numeric inputs should be numbers (modify utilities.c --> utilities.cpp?)
	[x] Check reading of images (i.e., do cfitsio routines return errors?)
	[] basic checks of config-file format
	[] parameter limits should be sane (lower limit < upper limit) and should bound initial value

[] Prototype use of FFTW "measure" mode
	-- locate moderately long PSF-using fit
	-- try it with fft_measure option
	-- try it with fft_measure option *and* fftw threading compiled in

[x] Prototype code for saving final residual image?
	-- should be deviates vector in ModelObject

[x] Prototype code for better pretty-printing of best-fit result
	-- should print function names and id separate function blocks
	-- could be similar/identical to output to config file

[x] Prototype code for saving best-fit parameter file
	-- suitable as intput to makeimage, or as starting point for new fitting attempt
	-- should also have timestamp, name of file fitted, command-line invocation,
	final chi^2, etc. (all as comments, with # in front)
	[x] Prototype saving information about fit (filename, command-line used, etc.)
	[x] Prototype saving best-fit parameters and functions
	
<>[x] Prototype code for getting non-function-related data from config file
	-- denote in config file via ALL_CAPS (e.g., GAIN, ORIGINAL_SKY, etc.)
	-- these should go *before* function listings
	-- for makeimage: IMAGE_SIZE_X, IMAGE_SIZE_Y (or _NCOLS, _NROWS)
	-- e.g., input image name; noise image name; mask image name gain; sky value; etc.
	-- read in and store in some kind of C++ dictionary/map?
		-- could use STL map, though we'd have to use, e.g., String keys and 
		double items (OK to store, e.g., image dimensions as double and then
		convert back to int later)
		-- would need to distinguish between text and numeric values (text -->
		fields of option structure, numeric values --> same OR --> entries
		in map)
		-- OR we could just add more entries to the options structure and
		use if-then logic to assign values properly
	[x] Modify ReadConfigFile() to identify first "X0" as start of function-
	definition section; then process pre-function lines (if any); then
	process function lines (as we currently do)
	<>[] Prototype processing of userConfigOptions data structure within imfit
	[] Prototype processing of userConfigOptions data structure within makeimage
	[] Modify profilefile_main.cpp to use new version(s) of ReadConfigFile

[x] Prototype code for outputting best-fit model profile in profilefit?

[] Prototype code for unit testing
	[x] Decide on C++ unit testing framework to use (CxxTest?)
	[x] Install C++ unit testing framework
	[x] Trial implementation of trival unit test
	[x] Plan out possible unit tests
	[x] Implement more sophisticated unit tests
	[] Add unit tests for ModelObject::CheckWeightVector
	[] Add more unit tests!

[.] Prototype code for generating an error "image" (1-D vector) from the input
image
	-- we'll assume that supplying a noise image overrides other options
	[x] implement command-line options to specify gain, readnoise
	[] prototype code to read header values from an image
	[] implement reading of header values from input image

[x] Prototype a generalized-ellipse function object

[x] Add option to makeimage (and imfit?) to output one image per *function*
	-- specialized option within ModelObject, to go through function list
	(associating correct X0,Y0) and generating and saving images (re-use
	the model-image vector?)
		-- public member function which takes image filename root, appends
		<function-name><integer> to filename

[x] Option for makeimage -- or separate program? -- to calculate "luminosities" 
   for individual functions?
	-- define a nominal very large image (user-specified size?), with function 
	centered in the middle, then integrate over the "image", summing up pixel 
	fluxes instead of storing them in an array

[.] Prototype option to use only a subset of input image
	-- Note: we can do this almost automatically with cfitsio!
	cfitsio allows you to pass a filename of the form "name.fits[x1:x2,y1:y2]"
	(or even "name.fits[2][x1:x2,*]") and it will automatically read the
	subfile *and* tweak the header to have correct NAXES and WCS
		-- potential issue: when we check for existence of image file, we'll
		need to check for filename.split("[") instead of just filename
	-- Allowed region-specifications (which we'll try to interpret):
		name.fits[x1:x2,y1:y2]
		name.fits[*,y1:y2]
		name.fits[x1:x2,*]
		name.fits[*,*]
		and variations of the above with [n] in front of the region specification
		(e.g., name.fits[3][x1:x2,y1:y2])
	[x] Prototype code to check for image-file existence, chopping off "[...]"
		if it exists (applies to image, mask, noise, and PSF; also to reference
		image in makeimage case)
	[x] Test command-line subsetting of image
	[x] Test config-file subsetting of image
		-- e.g., image.fits[x1:x2,y1:y2]
	[x] Investigate how to retain offset information and apply it to output params
		-- i.e., figure out coordinates of LL subset corner, then add appropriate
		offsets to output fitted parameters (all X0,Y0 pairs) so that results make
		sense on context of original image *and* are appropriate for an output
		config file (e.g. for makeimage use).
	[x] Prototype code for identifying and applying pixel offsets to output X0,Y0
	[x] Prototype code for applying pixel offsets to *input* X0,Y0 and their limits
		-- i.e., user specifies image[100:200,100:200]; we assume input X0,Y0 are
		relative to complete image, but these need to be transformed to section-
		relative coords for the actual fitting process -- and the same applies
		to the user-specified limits on X0 and Y0!
	[.] Prototype code for handling more complex image sections
		-- utilities.cpp: GetPixelStartCoords
		-- e.g., [3][x1:x2,y1:y2] or [3,x1:x2,y1:y2]
	[x] Prototype handling of "http://" or "ftp://" prefixes to file names, to
	   allow use of files on remote hosts (cfitsio capability)

[] Prototype use of magnitudes
	-- user specifies a zero point, and (somehow) magnitude parameters are
	converted to intensities
	-- possibly an extra flag for FunctionObject::Setup(), which tells the latter
	to expect magnitude values? (and we assume the individual sub-classes know
	which of their input parameters can be magnitudes)
		-- "flag" is zero point itself, with some default ridiculous value
		meaning non-magnitude?



** SPECULATIVE EXTRA STUFF:

* Future explorations of speeding things up:

	-- Try using FFTW real-to-complex + complex-to-real transforms (supposedly uses
	less memory *and* is faster)
		-- input (model) array is unchanged
		-- FFT of PSF and input model array are smaller than before
		-- complex product (input to c2r) array is "destroyed" by c2r inverse transform
	
	-- Try using in-place FFT transforms?
	
	-- Try rounding up to nearest power-of-two for FFT work
	
	-- Look into ways of parallelizing (e.g., via OpenMP) L-M process
		-- problem: computation of offset parameters implies multiple
		versions of ModelObject


ModelObject.ComputeDeviates *could* return an integer status value, which
myfunc [in imfit_main.cpp] could then pass on to mpfit (mpfit assigns the
return value of myfunc to "iflag", and if iflag < 0, then mpfit *terminates*
and that value becomes the status returned by mpfit.
	-- So we could, in principle, use this as a way of signalling that, e.g.,
wonky parameter values were passed to ModelObject.ComputeDeviates, or some
other bad thing had happened ...


Multi-Gaussian-Expansion mode:
	Python wrapper code to do n1--n2 Gaussian fits (n1, n2 = user input) and pick
the one with lowest reduced chi^2 and/or AIC or BIC.
	Note, however, that the IDL MGE approach (or at least Cappellari's 2002 version)
takes advantage of the nature of Gaussians to derive faster and more specialized
code ...



** Possible image class ideas:

Class attributes:
	- label (number)
	- label (string description)
	- vector of pixel values
	- nx
	- ny
	[- expanded sizes for PSF convolution?]
	- pixel scale (units per pixel, type of units ['arcsec', 'kpc', etc])
	- orientation ("telescope PA")
	- possible WCS? [something to be specified/added in subclass?]
	- filename?
	- associated PSF? associated Convolver object?



[X]** Possible model-based chi^2 statistics

Use model values instead of data values to determine sigma values.

AddErrorVector():
weightVector = input_pixel_vector
weightVector[z] = 1.0 / weightVector[z];

GenerateErrorVector():
  weightVector = (double *) calloc((size_t)nDataVals, sizeof(double));
  readNoise_sqrd = readNoise*readNoise/(effectiveGain*effectiveGain);

  for (int z = 0; z < nDataVals; z++) {
    // total flux = nCombined*(source + subtracted_sky_value)
    totalFlux = dataVector[z] + originalSky;
    if (totalFlux < 0.0)
      totalFlux = 0.0;
//    noise_squared = totalFlux/imageGain + readNoise_sqrd;
    noise_squared = totalFlux/effectiveGain + nCombined*readNoise_sqrd;
    // Note that we store 1/sigma instead of 1/sigma^2, since the chi^2 calculation in 
    // ChiSquared() [or the equivalent in mpfit.cpp) will square the individual terms
    weightVector[z] = 1.0 / sqrt(noise_squared);
  }

ApplyMask():
  if ( (weightValsSet) && (maskExists) ) {
    for (int z = 0; z < nDataVals; z++) {
      weightVector[z] = maskVector[z] * weightVector[z];
    }
    printf("ModelObject: mask vector applied to weight vector. ");
    printf("(%d valid pixels remain)\n", nValidDataVals);
  }


New function/method: UpdateWeightVector
	Called in e.g. ComputeDeviates(), ChiSquared(), after CreateModelImage is called
	
  if (doConvolution) {
      for (z = 0; z < nDataVals; z++) {
        if (maskVector[z] > 0) {
          // only update values that aren't masked out
          // (don't rely on previous weightVector[z] value, since sometimes model flux
          // might be zero for an unmasked pixel)
          iDataRow = z / nDataColumns;
          iDataCol = z - iDataRow*nDataColumns;
          zModel = nModelColumns*(nPSFRows + iDataRow) + nPSFColumns + iDataCol;
          totalFlux = modelVector[zModel] + originalSky;
          noise_squared = totalFlux/effectiveGain + nCombined*readNoise_sqrd;
          // POSSIBLE PROBLEM: if originalSky = model flux = read noise = 0, we'll have /0 error!
          weightVector[z] = 1.0 / sqrt(noise_squared);
        }
      }
  } else {
    // No convolution, so model image is same size & shape as data and weight images
    for (z = 0; z < nDataVals; z++) {
      if (maskVector[z] > 0) {
        // only update values that aren't masked out
        // (don't rely on previous weightVector[z] value, since sometimes model flux
        // might be zero for an unmasked pixel)
        totalFlux = modelVector[z] + originalSky;
        noise_squared = totalFlux/effectiveGain + nCombined*readNoise_sqrd;
        // POSSIBLE PROBLEM: if originalSky = model flux = read noise = 0, we'll have /0 error!
        weightVector[z] = 1.0 / sqrt(noise_squared);
    }
  }



[ ]** Possible modified-Cash-statistic (aka CSTAT) 
[Aug 2014 suggestion by David Streich]

POSSIBLE NAMES:
	Poisson Likelihood Ratio statistic
	chi^2_{\lambda}
	chi^2_{\lambda, p} -- to distinguish from other chi^2-like LR
	chi^2_{PLR}

POSSIBLE COMMAND-LINE FLAGS
	--plr (synomym for following?)
	--poisson-lr
	--poisson-mlr
	--chi2mlr [ambiguous]
	--chi2lr
	--chi2plr
	--chi2-poisson-mlr
	--poisson-loglr


[X]Pre-generate vector of d_i*log(d_i) - d_i values (mainly so we can catch possible
edge case of d_i = 0, for which the total quantity -> 0)

Similarities to Cash statistic:
	-- weight vector should be 0/1 only
	-- *don't* generate error vector
Differences from Cash statistic:
	-- L-M is OK  [handled in imfit_main.cpp]
	-- possible computation in ComputeDeviates
	-- pre-compute d_i*log(d_i) - d_i vector [modCashVector] extraCashTermsVector

QUESTION: Do we add a completely new, parallel flag and associated functions, or
do we add an additional flag which modifies current Cash-stat behavior?

[X]bool ModelObject::UsingCashStatistic -- 
	-- called by PrintResults

[X]<>Need separate flag-setting function from ModelObject::UseCashStatistic (i.e., something
called from main() to tell ModelObject what we're doing)
	-- could call UseCashStatistic internally to do weight-vector setup

[X]<>ModelObject: function to compute modified Cash stat
	-- modify CashStatistic() so it uses vector of d_i*log(d_i) - d_i

[X]<>ModelObject::ComputeDeviates
	-- new code to compute modified Cash stat deviates
	-- note that mpfit (and ModelObject::ChiSquared) computes mp_enorm(deviatesVector),
	which is  sqrt( Sum_i(chi_i^2) ) = sqrt( Sum_i(deviatesVector[i]^2) )

[X]<> Modifications to imfit_main.cpp to allow for and handle modified Cash stat

[X]<> BootstrapErrors
	-- uses input bool parameter usingCashStatistic to decide which minimizer
	to use (and what to say in print statements)

[X]bool useCashStatistic
	-- currently used just two places in ModelObject:
		1. Tested in FinalSetupForFitting() to see if we need to compute error vector
		2. Tested in GetFitStatistic() to see if we compute Cash stat or chi^2

NOTE:
Initial testing suggested that slightly different best-fit results when used on the
same image for modified Cash statistic vs standard Cash statistic. *However*, this
was an artifact of the combination of the value of the fit statistic and the
requested tolerance.  For standard Cash statistic, best-fit value ~ -2611601, while
best-fit modified Cash statistic value ~ 22778. So a default fractional tolerance
of 1.0e-8 allows more refining of the modified Cash-statistic value. When ftol is
set to 1.0e-9 for the standard Cash statistic case, the best-fitting parameter
values are ~ identical.  Identical values (within the display precision) are also
achieved for standard-Cash ftol = 1.0e-12 and modified-Cash ftol = 1.0e-10.

Currently [24 Aug 2014] we are using a kludgey solution for computing the
deviates vector, where we construct the deviates vector by taking the
square root of the individual likelihood terms:
	sqrt( 2 w_i (m_i - d_i log m_i )

[X]PROBLEM (24 Aug 2014):
Using current modified imfit with --modcash yields *negative* values for "modified
Cash statistic" when applied to "HAGGIS example" galaxies (/Beleriand/data/haggis/galaxy_decomposition/)
E.g., initial (function call 20) & final standard Cash values = -69010931.985299, -69089437.922793
Same for modified Cash values = -5231079.899640, -5309678.219723
OK, problem seems to only occur when we do PSF convolution (even when it's just
--chisquare-only calculation)
	--> SOMETHING WRONG WITH PSF-HANDLING FOR MOD-CASHSTAT CALCULATION
	extraCashTermsVector ?

ModelObject::CashStatistic --
        extraTerms = extraCashTermsVector[b];   // = 0 for standard Cash stat
TO:     extraTerms = extraCashTermsVector[z];   // = 0 for standard Cash stat

Also -- we hadn't actually completed the code for computing deviates in the case
of convolution + Cash stat!
FIXED: When m_i very close to d_i, mod-cash value is very, very small, and occasionally
(machine-precision/rounding errors?) slightly negative. FIX: take absolute
value before computing square root.

[X] Create tests for --modcash with and without PSF



* Notes for a King Model:

Larsen 1999 (main reference paper for ISHAPE) notes that ISHAPE has King profile
(alpha = 2, acc. to GALFIT 3.0 convention) has concentration (c = r_t/r_c) values
of 5, 15, 30, and 100

GALFIT 3 (according to the README file) has "empirical (modified)" King profile,
which is attributed to Elson (1999 = 10th Canary Islands Winter School of Astrophysics: 
Globular clusters, p. 209 - 248, with no links to actual text/paper in ADS!
which allows for arbitrary concentration (actually arbitrary values of r_t and r_c)
along with variable power-law index alpha [which is 2 in the standard definition]

Probably best to have the following parameters (in addition to ell, PA):
I0
r_c
c
alpha

(Makes it easier to specify a fixed concentration value)



** POSSIBLE ALTERNATE LIBRARY CODE:

Non-GPL FFT code:
http://anthonix.com/ffts/
	-- apparently nice and modern and fast; BSD license; restricted to power-of-2 arrays


Numerical integration code:

http://www.feynarts.de/cuba/ -- LGPL

http://ab-initio.mit.edu/wiki/index.php/Cubature -- GPL

Port of F77 QUADPACK to C:
http://www.crbond.com/scientific.htm -- "All downloadable software is offered freely and without  restriction"




** NOTES ON MULTIPLE CHI-SQUARE MINIMA:

With the default "tiny-exponential-disk" image (testimage_expdisk_tiny.fits, 4x4 pixels),
when fitting with the sum of two Gaussians (mpfit), there are at least two outcomes:
The whole image, row by row:
 10.687793 13.533528 10.687793 5.910574
 36.787945 100.000000 36.787945 13.533528
 10.687793 13.533528 10.687793 5.910574
 1.619414 1.831564 1.619414 1.142289

Initial params = {1.0, 1.0, 50.0, 1.0, 1.0, 1.0, 20.0, 0.5}
	==> converges to 
  CHI-SQUARE = 840.918927    (8 DOF)
        NPAR = 8
       NFREE = 8
     NPEGGED = 0
     NITER = 65
      NFEV = 594

  P[0] = 7.758884 +/- 0.000000
  P[1] = 7.195688 +/- 0.000000
  P[2] = -214.706757 +/- 0.000000
  P[3] = -0.603486 +/- 0.000000
  P[4] = 2.006787 +/- 0.009379
  P[5] = 2.000393 +/- 0.009380
  P[6] = 98.693523 +/- 0.991016
  P[7] = -0.623820 +/- 0.004437
The model image, row by row:
 7.417287 27.278586 7.680577 0.165562
 26.834042 98.687663 27.786564 0.598966
 7.432285 27.333742 7.696107 0.165897
 0.157599 0.579603 0.163193 0.003518

But with initial params = {1.0, 1.0, 50.0, 1.0, 1.5, 1.5, 20.0, 0.5}
	==> converges to 
  CHI-SQUARE = 400.776151    (8 DOF)
        NPAR = 8
       NFREE = 8
     NPEGGED = 0
     NITER = 200
      NFEV = 1850

  P[0] = 1.635329 +/- 0.305284
  P[1] = 2.001725 +/- 0.028840
  P[2] = 45.552504 +/- 15.606852
  P[3] = 0.763261 +/- 0.024219
  P[4] = 2.454461 +/- 3.663788
  P[5] = 1.995058 +/- 5.206057
  P[6] = 337.699031 +/- 56522.865669
  P[7] = 0.243732 +/- 9.799968
The model image, row by row:
 13.615329 17.189946 3.899910 0.158575
 32.214874 99.997723 36.789970 0.375200
 13.696221 17.289804 3.922025 0.159517
 1.046312 1.319919 0.299191 0.012186

Note that this is termination after maximum number of iterations (200)
Also note that chi-square is *lower*!
